---
title: "Introduction"
execute:
  freeze: true
editor:
  markdown:
    wrap: 72
output:
  html_document:
    df_print: tibble
---

```{r setup, include=FALSE, paged.print=TRUE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  paged.print=FALSE # this seemingly doesn't do anything
)
library(dplyr, warn.conflicts = FALSE)
library(guildai)


Sys.setenv(PATH = paste0(R.home("bin"), ":", Sys.getenv("PATH")))
for(v in c("VIRTUAL_ENV", "PYTHONHOME", "PYTHONPATH")) Sys.unsetenv(v)
Sys.setenv("RETICULATE_PYTHON" = "~/.virtualenvs/r-tensorflow/bin/python")
Sys.setenv("TF_CPP_MIN_LOG_LEVEL" = "2")

if(!interactive())
  unlink(".guild", recursive = TRUE)

# help knitr capture stdout 
guild_run <- function(..., flags = NULL, echo = TRUE) {
  stdout <- stderr <- echo
  flags$.fast <- interactive()
  out <- guildai::guild_run(..., flags = flags, stdout = stdout, stderr = stderr)
  try(writeLines(out))
  invisible(out)
}
Sys.setenv("GUILD_HOME" = normalizePath(file.path(getwd(), ".guild"),
                                        mustWork = FALSE))

```

<!-- a nice screenshot image here-->

*guildai* provides a suite of tools for tracking, visualizing, and
managing training runs and experiments. The {guildai} R package is a
successor to the {tfruns} package.

-   Track the hyperparameters, metrics, output, and source code of every
    training run.

-   Compare hyperparmaeters and metrics across runs to find the best
    performing model.

-   Automatically generate reports to visualize individual training runs
    or comparisons between runs.

-   No changes to source code required.

## Installation

The R package provides an interface to [guildai
core](https://guild.ai/). The R package will install guildai core on
first use, or you can call `install_guild()` to customize the
installation. You can install the **guildai** package from CRAN as
follows:

```{r, eval = FALSE}
install.packages("guildai")
guildai::install_guild()
```

{guildai} can be used with any machine learning framework, or even no
framework at all. For this introductory example, we'll start with a
Keras model applied to the fashion mnist dataset.

If you've not used Keras from R before and you want to follow along on
your machine, you can install it like this:

```{r, eval = FALSE}
install.packages("keras")
library(keras)
install_keras()
```

## Hello World

To start, we'll setup a sample project folder with one script.

```{r}
file.copy(system.file("examples/fashion-mnist.R", package = "guildai"),
          ".", overwrite = TRUE)
```

Here is what the training script looks like:

```{r}
#| file: !expr "'fashion-mnist.R'"
#| eval: false
```

To train a model with **guildai**, just use the `guild_run()` function
in place of the `source()` function to execute your R script. For
example:

```{r}
guild_run("fashion-mnist.R")
```

By default, the output stream of the run will be shown at the R console.
After launching a run, you can launch an application to view your runs
with `guild_view()`. From the Guild View application, you can also
visualize run results using tensorboard.

```{r, eval = FALSE}
guild_view()
```

<!-- Include a screenshot here -->

You can also retrieve a data frame with information about the run with
`ls_runs()`:

```{r}
run <- ls_runs()
str(run)
```

`ls_runs()` returns a data frame with information about runs. In our
sample project, we've launched one run, so `ls_runs()` returns a 1-row
data frame.

`guild_view()` and `ls_runs()` provide two convenient ways to gather and
present the information from runs. Importantly however, all the
information about the run is stored as plain files.

```{r}
fs::dir_tree(run$dir, all=TRUE)
```

A run can also be used to generate a summary report, a paramaterized
quarto document:

```{r eval=FALSE}
view_run_report(run$id)
```

### Comparing Runs

Let's make a couple of changes to our training script to see if we can
improve model performance. We'll change the number of units in our first
dense layer to 128, change the `learning_rate` from 0.001 to 0.003 and
run 30 rather than 20 `epochs`. After making these changes to the source
code we re-run the script using `guild_run()` as before:

```{r, echo = FALSE}
guildai:::modify_r_file_flags("fashion-mnist.R", 
  list(units = 128, learning_rate = 0.003, epochs = 30),
  overwrite = TRUE)
```

```{r}
guild_run("fashion-mnist.R")
```

This will also show us a report summarizing the results of the run, but
what we are really interested in is a comparison between this run and
the previous one.

The individual metrics `test_loss` and `test_accuracy` are visible in
the comparison table in the Guild View application, as well as in the We
can view a comparison via the `view_runs_diff()` function:

```{r, eval = FALSE}
view_runs_diff()
```

The comparison report shows the model attributes and metrics
side-by-side, as well as differences in the source code and output of
the training script.

Note that `view_runs_diff()` will by default compare the last two runs,
however you can pass any two run ids you like to be compared.

## Flags

Flags are a form of run inputs, or paramaterization. The action we just
did, of modifying `learning_rate`, `epochs` and `units` values in the
script before launching the second run, can be handled for us by
`guild_run()` using the *flags* interface.

By default, top-level assignments of scalar literals in an R script are
identified by guild as run flags that can be modified per-run. You can
quickly see what flags are available in an R script by passing
`--help-op` (more on this later).

```{r}
guild_run("fashion-mnist.R", "--help-op")
```

To launch a run with different flag values, we can do this:

```{r, include=FALSE}
guild_run("fashion-mnist.R", 
          flags = list(learning_rate = 0.001, 
                       units = 256))
```

Now, when we inspect the run sources with `view_runs_diff()`, we see
that the source files associated with the run have the updated flag
values, as if we had modified them manually.

The flags interface is useful for hyperparamater optimization. At it's
simplest, we can just iterate over the set of flag values we want:

```{r, eval = FALSE}
for (learning_rate in c(0.001, 0.003))
  guild_run("fashion-mnist.R", c(learning_rate = learning_rate),
            wait = FALSE)
```

Here `wait = FALSE` means that the `guild_run()` call launches the run
process and returns immediately. This is an easy way to launch multiple
training runs in parallel. We can view the progress and real-time
outputs of our runs with `guild_view()`, where their status ("training"
or "completed").

Alternatively, we can pass multiple values for each flag, and guild will
automatically expand the combinations to a grid search. For example,
this will launch 4 training runs, with each combination of flag values:

```{r}
guild_run("fashion-mnist.R", 
          flags = list(learning_rate = c(0.001, 0.003),
                       units = c(128, 256)))
```

For more precision, we can pass a dataframe of flags values, with each
row corresponding to a run.

```{r, eval = TRUE}
flags_df <- expand.grid(learning_rate = c(0.001, 0.003),
                        units = c(128, 256))
flags_df
```

```{r, eval = FALSE}
guild_run("fashion-mnist.R", flags = flags_df)
```

### Flag annotations

We can optionally supply additional metadata about individual flags by
placing hashpipe yaml annotations above the flag expression. For
example, we can update our "fashion-mnist.R" script with the following
lines:

    #| description: size of first layer.
    #| min: 16
    #| max: 256
    units <- 32

    #| description: Activation function to use.
    #| choices: [relu, sigmoid, tanh]
    activation <- "relu"

Now, the `description`s and constraints will appear in `--help-op` and
related locations.

### Flag destinations

As a project grows, it's helpful to be able to move flag definitions out
of the main R script. To do so, you can include a `flags-dest` in the
frontmatter of the R script, specifying the file path (relative to the
project directory) of the file where guild should place the flag values.
Then you can read in the flags using `source()` or similar.

    #| flags-dest: ./flags.R

    FLAGS <- envir::include("flags.R", new.env())

YAML files are also supported as a flags destination:

    #| flags-dest: ./flags.yml

    FLAGS <- yaml::read_yaml("flags.yml")

### Retreiving Run Flags

The flags and flag values associated with each runs are returned by
`ls_runs()` as a nested dataframe under `flags`.

```{r paged.print=FALSE}
runs <- ls_runs()
runs %>%
  select(shortId, flags) #%>% tidyr::unnest(flags, names_sep = "_")
```

## Scalars

The counterpart to run `flags` are run `scalars`. Where as `flags` are a
type of run input, scalars are run outputs identified by Guild as
meaningful to track.

```{r paged.print=FALSE}
runs %>%
  select(shortId, scalars)

glimpse(runs$scalars[[1]])
```

```{r eval=FALSE, include=FALSE}
guild_view()
view_run_report()
```

Here we see that guild has automatically identified `test_accuracy` and
`test_loss` as run scalar outputs. By default, any lines printed to
standard output during the run with the patten `"key: numeric-value"`
are recorded by guild as `scalars`. If you are printing values for the
same scalar `key` multiple times during a run (e.g, `loss` during a
training loop), then be sure to also print a `step` scalar in between,
to enable guild to track history (and enable visualization of the run
metrics with tensorboard).

Alternatively, if your run process produces tfevent records directly
(e.g., `keras::callback_tensorboard("./logs")`), then those
automatically identified by guild a run scalars, and included in
`ls_runs()` (and `guild_view()`, and tensorboard and other run views).

`ls_runs()` by default only returns a summary of run scalars, but the
full scalar history can also be accessed from R directly:

```{r}
ls_runs(scalars = TRUE)
```

## Using Flags and Scalars Together

We can use guild to explore what impact `units` has on `test_accuracy`.

```{r}
units <- (2 ^ (4:11)) %>% c(diff(., 2)) %>% sort()
units 
guild_run("fashion-mnist.R", 
          flags = list(units = units),
          echo = FALSE)
```

We can see compare run flags and run scalars from R:

```{r paged.print=FALSE}
runs <- ls_runs(paste0("1:", length(units))) # last 8 runs

df <- runs %>%
  select(flags, scalars) %>%
  rowwise() %>%
  mutate(across(scalars, function(run_scalars_df) {
    run_scalars_df %>%
      select(tag, last_val) %>%
      tidyr::pivot_wider(names_from = tag,
                         values_from = last_val)
  })) %>%
  tidyr::unnest(c(flags, scalars)) %>%
  arrange(units)

df
```

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
ggplot(df, aes(x = units, y = test_accuracy)) +
  geom_point() + geom_smooth()
```

### Addin

The **guildai** package installs an RStudio IDE addin which provides
quick access to frequently used functions from the Addins menu:

Note that you can use **Tools** -\> **Modify Keyboard Shortcuts** within
RStudio to assign a keyboard shortcut to one or more of the addin
commands.

### Background Training

Since training runs can become quite lengthy, it's often useful to run
them in the background in order to keep the R console free for other
work. You can launch a guild run without blocking the R console by
specifying `guild_run(wait = FALSE)` in the call. You can then view
real-time outputs from your run(s) using `guild_view()`.

Alternatively, you can launch training runs in the terminal pane:

    Rscript -e 'guildai::guild_run("train.R")'

If you are not running within RStudio then you can of course use a
system terminal window for background training.
